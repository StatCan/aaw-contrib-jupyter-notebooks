{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Queries with Minio (& PowerBI)\n",
    "\n",
    "Minio implements the [S3 SELECT API](https://docs.min.io/docs/minio-select-api-quickstart-guide.html). It is not effective for creating joins or other relational database tricks, but it's phenomenal at extracting exactly the data that you need, so that your queries are blazingly fast. \n",
    "\n",
    "\n",
    "For reference on how to use this SQL flavour, look at\n",
    "\n",
    "[The AWS reference](https://docs.aws.amazon.com/AmazonS3/latest/dev/s3-glacier-select-sql-reference-select.html)\n",
    "\n",
    "\n",
    "*Note: Amazon S3 Select does not support whole-object compression for Parquet objects.*\n",
    "[Source](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.select_object_content)\n",
    "\n",
    "**NOTE: The examples here use JSON, but CSV is better suited to large datasets, performing 10x faster in my experiment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import daaas_storage_boto3 as storage\n",
    "\n",
    "s3 = storage.get_minimal_client()\n",
    "\n",
    "BUCKET = \"shared\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast SQL Extractions and pandas (can be used with PowerBI)\n",
    "\n",
    "Minio implements the S3 Select API, which reads a minimal amount of data off of disk. This makes the queries very fast, even on large tables. Also, you can read the data straight out of a file, without creating or managing a complex database.\n",
    "\n",
    "**PowerBI**: You can use these snippets to load pandas dataframes into PowerBI. Check out [the PowerBI tutorial](https://docs.microsoft.com/en-us/power-bi/connect-data/desktop-python-scripts). **note:** this only works with pandas, not arrow. So use `storage.pandas_from_json`. **Do not use** `storage.arrow_from_json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query your data with SQL (.csv.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "r = s3.select_object_content(\n",
    "    Bucket=BUCKET,\n",
    "    Key='/blair-drummond/sql-example/TotalPopulation.csv.gz',\n",
    "    ExpressionType='SQL',\n",
    "    # Note, there's no ';' at the end.\n",
    "    Expression=\"\"\"\n",
    "    SELECT PopTotal,PopDensity FROM s3object s \n",
    "    WHERE s.Location like '%Canada%'\n",
    "    \"\"\",\n",
    "    InputSerialization={\n",
    "        'CSV': {\n",
    "            # Use this if your CSV file has a header. Else set to \"NONE\".\n",
    "            \"FileHeaderInfo\": \"USE\",\n",
    "            'RecordDelimiter': '\\n',\n",
    "            'FieldDelimiter': ',',\n",
    "        },\n",
    "        # Remove this if the file isn't compressed.\n",
    "        'CompressionType': 'GZIP',\n",
    "    },\n",
    "    OutputSerialization={'JSON': {}},\n",
    "    #OutputSerialization={'CSV': {'RecordDelimiter': '\\n', 'FieldDelimiter': ','}},\n",
    ")\n",
    "\n",
    "df = storage.pandas_from_json(r)\n",
    "#df = storage.pandas_from_csv(r)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query your data with SQL (.parquet)\n",
    "\n",
    "**NOTE: If you're running this on PowerBI, you'll need either pyarrow or fastparquet installed.** \n",
    "\n",
    "**Note:** You should not compress your parquet files[^1]!!! They can be larger compressed, and the S3 Select API does not support querying them.\n",
    "\n",
    "[^1]: Unless you use SNAPPY. But BZIP2 and GZIP are not supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "r = s3.select_object_content(\n",
    "    Bucket=BUCKET,\n",
    "    Key='/blair-drummond/sql-example/TotalPopulation.parquet',\n",
    "    ExpressionType='SQL',\n",
    "    Expression=\"SELECT * FROM s3object s WHERE s.Location like '%Canada%'\",\n",
    "    InputSerialization={\n",
    "        'Parquet': {},\n",
    "        'CompressionType': 'NONE',\n",
    "    },\n",
    "    OutputSerialization={'JSON': {}},\n",
    ")\n",
    "\n",
    "df = storage.pandas_from_json(r)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query your data with SQL (.csv)\n",
    "\n",
    "Note, you'll probably get **significant** storage savings if you compress your csv files. (Read: 10gb -> 500mb, for example). So if it's under your control, it's recommended to use gzip. The S3 Select API that we're using also has some support for BZIP2. *(You can also use SNAPPY on `.parquet` files)*\n",
    "\n",
    "[S3 Select Compression Support](https://aws.amazon.com/about-aws/whats-new/2018/09/amazon-s3-announces-new-features-for-s3-select/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "r = s3.select_object_content(\n",
    "    Bucket=BUCKET,\n",
    "    Key='/blair-drummond/sql-example/TotalPopulation.csv',\n",
    "    ExpressionType='SQL',\n",
    "    # Note, there's no ';' at the end.\n",
    "    Expression=\"\"\"\n",
    "    SELECT PopTotal,PopDensity FROM s3object s \n",
    "    WHERE s.Location like '%Canada%'\n",
    "    \"\"\",\n",
    "    InputSerialization={\n",
    "        'CSV': {\n",
    "            # Use this if your CSV file has a header. Else set to \"NONE\".\n",
    "            \"FileHeaderInfo\": \"USE\",\n",
    "        },\n",
    "        # Remove this if the file isn't compressed.\n",
    "        # 'CompressionType': 'GZIP',\n",
    "    },\n",
    "    # JSON is easier to work with than csv, unless you\n",
    "    # have a massive amount of data.\n",
    "    OutputSerialization={'JSON': {}},\n",
    ")\n",
    "\n",
    "df = storage.pandas_from_json(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: Json v.s. CSV\n",
    "\n",
    "JSON transmits more data than CSV, so **if performance is key, use csv**.\n",
    "\n",
    "**The disadvantage of CSV, is that the S3 API for CSV doesn't return you column names.**\n",
    "\n",
    "However, you can run a small JSON query, then manually stitch together the column names.\n",
    "\n",
    "Compare the times below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "r = s3.select_object_content(\n",
    "    Bucket=BUCKET,\n",
    "    Key='/blair-drummond/sql-example/TotalPopulation.csv.gz',\n",
    "    ExpressionType='SQL',\n",
    "    # Note, there's no ';' at the end.\n",
    "    Expression=\"\"\"\n",
    "    SELECT PopTotal,PopDensity FROM s3object s \n",
    "    \"\"\",\n",
    "    InputSerialization={\n",
    "        'CSV': {\n",
    "            # Use this if your CSV file has a header. Else set to \"NONE\".\n",
    "            \"FileHeaderInfo\": \"USE\",\n",
    "            'RecordDelimiter': '\\n',\n",
    "            'FieldDelimiter': ',',\n",
    "        },\n",
    "        # Remove this if the file isn't compressed.\n",
    "        'CompressionType': 'GZIP',\n",
    "    },\n",
    "    OutputSerialization={'JSON': {}},\n",
    "    #OutputSerialization={'CSV': {'RecordDelimiter': '\\n', 'FieldDelimiter': ','}},\n",
    ")\n",
    "\n",
    "df = storage.pandas_from_json(r)\n",
    "#df = storage.pandas_from_csv(r)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "r = s3.select_object_content(\n",
    "    Bucket=BUCKET,\n",
    "    Key='/blair-drummond/sql-example/TotalPopulation.csv.gz',\n",
    "    ExpressionType='SQL',\n",
    "    # Note, there's no ';' at the end.\n",
    "    Expression=\"\"\"\n",
    "    SELECT PopTotal,PopDensity FROM s3object s \n",
    "    \"\"\",\n",
    "    InputSerialization={\n",
    "        'CSV': {\n",
    "            # Use this if your CSV file has a header. Else set to \"NONE\".\n",
    "            \"FileHeaderInfo\": \"USE\",\n",
    "            'RecordDelimiter': '\\n',\n",
    "            'FieldDelimiter': ',',\n",
    "        },\n",
    "        # Remove this if the file isn't compressed.\n",
    "        'CompressionType': 'GZIP',\n",
    "    },\n",
    "    #OutputSerialization={'JSON': {}},\n",
    "    OutputSerialization={'CSV': {'RecordDelimiter': '\\n', 'FieldDelimiter': ','}},\n",
    ")\n",
    "\n",
    "#df = storage.pandas_from_json(r)\n",
    "df = storage.pandas_from_csv(r)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond Pandas: Arrow v.s. Pandas, CSV v.s. Parquet\n",
    "\n",
    "*Apache Arrow* is a newer tool designed for efficient data storage and retrieval. It's how Pandas opens `.parquet` files. We're going to do some benchmarking here, and we'll look at an experiment with the following variables.\n",
    "\n",
    "1. File Format\n",
    "    - `.csv` \n",
    "    - `.csv.gz` \n",
    "    - `.parquet`\n",
    "    \n",
    "2. Query type\n",
    "    - *Row Extraction*\n",
    "    - *Column Extraction*\n",
    "    \n",
    "3. Python Tool\n",
    "    - `pyarrow`\n",
    "    - `pandas`\n",
    "\n",
    "\n",
    "We're going to test every combination of these, to see how they work with eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "###           Arrow versus Pandas            ### \n",
    "################################################\n",
    "import time\n",
    "def timing(f):\n",
    "    \"\"\" Discard the output of the function, but get the time. \"\"\"\n",
    "    def wrap(*args):\n",
    "        time1 = time.time()\n",
    "        f(*args)\n",
    "        time2 = time.time()\n",
    "        # milliseconds\n",
    "        ms = (time2-time1)*1000.0\n",
    "        return ms\n",
    "    return wrap\n",
    "\n",
    "funcs = {\n",
    "    'arrow'  : timing(storage.arrow_from_json),    \n",
    "    'pandas' : timing(storage.pandas_from_json)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "###       Row versus Column Operations       ###\n",
    "################################################\n",
    "\n",
    "## Query is adjusted so that roughly \n",
    "## the same amount of data is scanned.\n",
    "sql = { \n",
    "        # nrow = 4420, ncol = 2;   nrow*ncol = 8840\n",
    "        'column' : \"\"\"\n",
    "        SELECT PopTotal,PopDensity FROM s3object s \n",
    "        LIMIT 4420\n",
    "        \"\"\",\n",
    "        # nrow = 884, ncol = 10;  nrow*ncol = 8840\n",
    "        'row' : \"\"\"\n",
    "        SELECT * FROM s3object s \n",
    "        WHERE s.Location like '%Canada%'\n",
    "        \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "###                File Format               ###\n",
    "################################################\n",
    "\n",
    "## Note that csv.gz is smaller than parquet!\n",
    "\n",
    "def exp_csv(sql_query):\n",
    "    return s3.select_object_content(\n",
    "        Bucket=BUCKET,\n",
    "        # File size = 21 mb\n",
    "        Key='/blair-drummond/sql-example/TotalPopulation.csv',\n",
    "        ExpressionType='SQL',\n",
    "        Expression=sql_query,\n",
    "        InputSerialization={'CSV': {\"FileHeaderInfo\": \"USE\"}},\n",
    "        OutputSerialization={'JSON': {}}\n",
    "    )\n",
    "\n",
    "\n",
    "def exp_csv_gz(sql_query):\n",
    "    return s3.select_object_content(\n",
    "        Bucket=BUCKET,\n",
    "        # File size = 5.6 mb\n",
    "        Key='/blair-drummond/sql-example/TotalPopulation.csv.gz',\n",
    "        ExpressionType='SQL',\n",
    "        Expression=sql_query,\n",
    "        InputSerialization={\n",
    "            'CSV': {\"FileHeaderInfo\": \"USE\"},\n",
    "            'CompressionType': 'GZIP',\n",
    "        },\n",
    "        OutputSerialization={'JSON': {}}\n",
    "    )\n",
    "\n",
    "\n",
    "def exp_parquet(sql_query):\n",
    "    return s3.select_object_content(\n",
    "        Bucket=BUCKET,\n",
    "        # File size = 6.8 mb\n",
    "        Key='/blair-drummond/sql-example/TotalPopulation.parquet',\n",
    "        ExpressionType='SQL',\n",
    "        Expression=sql_query,\n",
    "        InputSerialization={'Parquet': {}},\n",
    "        OutputSerialization={'JSON': {}}\n",
    "    )\n",
    "\n",
    "formats = {\n",
    "    'csv'     : exp_csv,\n",
    "    'csv.gz'  : exp_csv_gz,\n",
    "    'parquet' : exp_parquet   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### By Column\n",
    "\n",
    "col_exp = lambda backend,file: funcs[backend](formats[file](sql['column']))\n",
    "\n",
    "cols = pd.DataFrame({\n",
    "    'csv'     : [ col_exp('pandas', 'csv'),     col_exp('arrow', 'csv')      ],\n",
    "    'csv.gz'  : [ col_exp('pandas', 'csv.gz'),  col_exp('arrow', 'csv.gz')   ],\n",
    "    'parquet' : [ col_exp('pandas', 'parquet'), col_exp('arrow', 'parquet')  ]\n",
    "    }, index=['pandas', 'arrow'])\n",
    "\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### By Row\n",
    "\n",
    "row_exp = lambda backend,file: funcs[backend](formats[file](sql['row']))\n",
    "\n",
    "rows = pd.DataFrame({\n",
    "    'csv'     : [ row_exp('pandas', 'csv'),     row_exp('arrow', 'csv')      ],\n",
    "    'csv.gz'  : [ row_exp('pandas', 'csv.gz'),  row_exp('arrow', 'csv.gz')   ],\n",
    "    'parquet' : [ row_exp('pandas', 'parquet'), row_exp('arrow', 'parquet')  ]\n",
    "    }, index=['pandas', 'arrow'])\n",
    "\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: I think Parquet will probably perform much better as the file size increases. Our files here are pretty small.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: arrow > pandas.\n",
    "\n",
    "This experiment done with a very small dataset, but there are two observations:\n",
    "\n",
    "1. **Arrow is faster than pandas in every case**.\n",
    "2. **Scanning columns is WAY faster than scanning Rows**.\n",
    "\n",
    "Also, note that while `csv.gz` is slightly slower than `csv`, the `csv.gz` files are `1/4` the size in storage. For large files, this will translate to faster transfer speeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
