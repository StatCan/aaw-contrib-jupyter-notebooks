{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Queries with Minio (& PowerBI)\n",
    "\n",
    "Minio implements the [S3 SELECT API](https://docs.min.io/docs/minio-select-api-quickstart-guide.html). It is not effective for creating joins or other relational database tricks, but it's phenomenal at extracting exactly the data that you need, so that your queries are blazingly fast. \n",
    "\n",
    "\n",
    "For reference on how to use this SQL flavour, look at\n",
    "\n",
    "[The AWS reference](https://docs.aws.amazon.com/AmazonS3/latest/dev/s3-glacier-select-sql-reference-select.html)\n",
    "\n",
    "\n",
    "*Note: Amazon S3 Select does not support whole-object compression for Parquet objects.*\n",
    "[Source](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.select_object_content)\n",
    "\n",
    "**NOTE: The examples here use JSON, but CSV is better suited to large datasets, performing 10x faster in my experiment.**\n",
    "\n",
    "**NOTE: The .Parquet example is commented out. Parquet is DISABLED by default since hostile crafted input can easily crash the server.**\n",
    "[Source](https://docs.min.io/docs/minio-select-api-quickstart-guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T18:34:33.009756Z",
     "iopub.status.busy": "2021-06-16T18:34:33.009433Z",
     "iopub.status.idle": "2021-06-16T18:34:55.711631Z",
     "shell.execute_reply": "2021-06-16T18:34:55.710414Z",
     "shell.execute_reply.started": "2021-06-16T18:34:33.009669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-4.0.1-cp38-cp38-manylinux2014_x86_64.whl (21.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.9 MB 124 kB/s  eta 0:00:01       | 4.3 MB 40.8 MB/s eta 0:00:01     |███████████▉                    | 8.1 MB 40.8 MB/s eta 0:00:01     |█████████████████               | 11.7 MB 68.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.8/site-packages (from pyarrow) (1.20.2)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-4.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T18:34:55.713290Z",
     "iopub.status.busy": "2021-06-16T18:34:55.713068Z",
     "iopub.status.idle": "2021-06-16T18:34:59.110481Z",
     "shell.execute_reply": "2021-06-16T18:34:59.109408Z",
     "shell.execute_reply.started": "2021-06-16T18:34:55.713265Z"
    }
   },
   "outputs": [],
   "source": [
    "import daaas_storage_boto3 as storage\n",
    "\n",
    "s3 = storage.get_standard_client()\n",
    "\n",
    "BUCKET = \"shared\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast SQL Extractions and pandas (can be used with PowerBI)\n",
    "\n",
    "Minio implements the S3 Select API, which reads a minimal amount of data off of disk. This makes the queries very fast, even on large tables. Also, you can read the data straight out of a file, without creating or managing a complex database.\n",
    "\n",
    "**PowerBI**: You can use these snippets to load pandas dataframes into PowerBI. Check out [the PowerBI tutorial](https://docs.microsoft.com/en-us/power-bi/connect-data/desktop-python-scripts). **note:** this only works with pandas, not arrow. So use `storage.pandas_from_json`. **Do not use** `storage.arrow_from_json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query your data with SQL (.csv.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T18:34:59.113116Z",
     "iopub.status.busy": "2021-06-16T18:34:59.112644Z",
     "iopub.status.idle": "2021-06-16T18:35:00.531071Z",
     "shell.execute_reply": "2021-06-16T18:35:00.530252Z",
     "shell.execute_reply.started": "2021-06-16T18:34:59.113056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.6 ms, sys: 12.9 ms, total: 33.4 ms\n",
      "Wall time: 1.39 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created Dataframe with dimensions: (nrow, ncol) = (884, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PopTotal</th>\n",
       "      <th>PopDensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13733.398</td>\n",
       "      <td>1.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14078.449</td>\n",
       "      <td>1.548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14445.453</td>\n",
       "      <td>1.589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14834.905</td>\n",
       "      <td>1.631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15245.416</td>\n",
       "      <td>1.677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PopTotal  PopDensity\n",
       "0  13733.398       1.510\n",
       "1  14078.449       1.548\n",
       "2  14445.453       1.589\n",
       "3  14834.905       1.631\n",
       "4  15245.416       1.677"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r = s3.select_object_content(\n",
    "    Bucket=BUCKET,\n",
    "    Key='/blair-drummond/sql-example/TotalPopulation.csv.gz',\n",
    "    ExpressionType='SQL',\n",
    "    # Note, there's no ';' at the end.\n",
    "    Expression=\"\"\"\n",
    "    SELECT PopTotal,PopDensity FROM s3object s \n",
    "    WHERE s.Location like '%Canada%'\n",
    "    \"\"\",\n",
    "    InputSerialization={\n",
    "        'CSV': {\n",
    "            # Use this if your CSV file has a header. Else set to \"NONE\".\n",
    "            \"FileHeaderInfo\": \"USE\",\n",
    "            'RecordDelimiter': '\\n',\n",
    "            'FieldDelimiter': ',',\n",
    "        },\n",
    "        # Remove this if the file isn't compressed.\n",
    "        'CompressionType': 'GZIP',\n",
    "    },\n",
    "    OutputSerialization={'JSON': {}},\n",
    "    #OutputSerialization={'CSV': {'RecordDelimiter': '\\n', 'FieldDelimiter': ','}},\n",
    ")\n",
    "\n",
    "df = storage.pandas_from_json(r)\n",
    "#df = storage.pandas_from_csv(r)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query your data with SQL (.parquet)\n",
    "\n",
    "**NOTE: If you're running this on PowerBI, you'll need either pyarrow or fastparquet installed.** \n",
    "\n",
    "**Note:** You should not compress your parquet files[^1]!!! They can be larger compressed, and the S3 Select API does not support querying them.\n",
    "\n",
    "**NOTE: Cannot run SQL queries on Parquet. Parquet is DISABLED by default since hostile crafted input can easily crash the server.**\n",
    "[Source](https://docs.min.io/docs/minio-select-api-quickstart-guide.html)\n",
    "\n",
    "[^1]: Unless you use SNAPPY. But BZIP2 and GZIP are not supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query your data with SQL (.csv)\n",
    "\n",
    "Note, you'll probably get **significant** storage savings if you compress your csv files. (Read: 10gb -> 500mb, for example). So if it's under your control, it's recommended to use gzip. The S3 Select API that we're using also has some support for BZIP2. *(You can also use SNAPPY on `.parquet` files)*\n",
    "\n",
    "[S3 Select Compression Support](https://aws.amazon.com/about-aws/whats-new/2018/09/amazon-s3-announces-new-features-for-s3-select/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T19:12:40.779008Z",
     "iopub.status.busy": "2021-07-05T19:12:40.778770Z",
     "iopub.status.idle": "2021-07-05T19:12:41.849083Z",
     "shell.execute_reply": "2021-07-05T19:12:41.848549Z",
     "shell.execute_reply.started": "2021-07-05T19:12:40.778985Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.2 ms, sys: 1.32 ms, total: 23.5 ms\n",
      "Wall time: 1.06 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created Dataframe with dimensions: (nrow, ncol) = (884, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PopTotal</th>\n",
       "      <th>PopDensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13733.398</td>\n",
       "      <td>1.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14078.449</td>\n",
       "      <td>1.548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14445.453</td>\n",
       "      <td>1.589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14834.905</td>\n",
       "      <td>1.631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15245.416</td>\n",
       "      <td>1.677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PopTotal  PopDensity\n",
       "0  13733.398       1.510\n",
       "1  14078.449       1.548\n",
       "2  14445.453       1.589\n",
       "3  14834.905       1.631\n",
       "4  15245.416       1.677"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r = s3.select_object_content(\n",
    "    Bucket=BUCKET,\n",
    "    Key='/blair-drummond/sql-example/TotalPopulation.csv',\n",
    "    ExpressionType='SQL',\n",
    "    # Note, there's no ';' at the end.\n",
    "    Expression=\"\"\"\n",
    "    SELECT PopTotal,PopDensity FROM s3object s \n",
    "    WHERE s.Location like '%Canada%'\n",
    "    \"\"\",\n",
    "    InputSerialization={\n",
    "        'CSV': {\n",
    "            # Use this if your CSV file has a header. Else set to \"NONE\".\n",
    "            \"FileHeaderInfo\": \"USE\",\n",
    "        },\n",
    "        # Remove this if the file isn't compressed.\n",
    "        # 'CompressionType': 'GZIP',\n",
    "    },\n",
    "    # JSON is easier to work with than csv, unless you\n",
    "    # have a massive amount of data.\n",
    "    OutputSerialization={'JSON': {}},\n",
    ")\n",
    "\n",
    "df = storage.pandas_from_json(r)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: Json v.s. CSV\n",
    "\n",
    "JSON transmits more data than CSV, so **if performance is key, use csv**.\n",
    "\n",
    "**The disadvantage of CSV, is that the S3 API for CSV doesn't return you column names.**\n",
    "\n",
    "However, you can run a small JSON query, then manually stitch together the column names.\n",
    "\n",
    "Compare the times below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T19:18:44.320944Z",
     "iopub.status.busy": "2021-07-05T19:18:44.320698Z",
     "iopub.status.idle": "2021-07-05T19:18:47.049551Z",
     "shell.execute_reply": "2021-07-05T19:18:47.048978Z",
     "shell.execute_reply.started": "2021-07-05T19:18:44.320921Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 573 ms, sys: 164 ms, total: 737 ms\n",
      "Wall time: 2.72 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created Dataframe with dimensions: (nrow, ncol) = (280932, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PopTotal</th>\n",
       "      <th>PopDensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7752.117</td>\n",
       "      <td>11.874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7840.151</td>\n",
       "      <td>12.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7935.996</td>\n",
       "      <td>12.156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8039.684</td>\n",
       "      <td>12.315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8151.316</td>\n",
       "      <td>12.486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PopTotal  PopDensity\n",
       "0  7752.117      11.874\n",
       "1  7840.151      12.009\n",
       "2  7935.996      12.156\n",
       "3  8039.684      12.315\n",
       "4  8151.316      12.486"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r = s3.select_object_content(\n",
    "    Bucket=BUCKET,\n",
    "    Key='/blair-drummond/sql-example/TotalPopulation.csv.gz',\n",
    "    ExpressionType='SQL',\n",
    "    # Note, there's no ';' at the end.\n",
    "    Expression=\"\"\"\n",
    "    SELECT PopTotal,PopDensity FROM s3object s \n",
    "    \"\"\",\n",
    "    InputSerialization={\n",
    "        'CSV': {\n",
    "            # Use this if your CSV file has a header. Else set to \"NONE\".\n",
    "            \"FileHeaderInfo\": \"USE\",\n",
    "            'RecordDelimiter': '\\n',\n",
    "            'FieldDelimiter': ',',\n",
    "        },\n",
    "        # Remove this if the file isn't compressed.\n",
    "        'CompressionType': 'GZIP',\n",
    "    },\n",
    "    OutputSerialization={'JSON': {}},\n",
    "    #OutputSerialization={'CSV': {'RecordDelimiter': '\\n', 'FieldDelimiter': ','}},\n",
    ")\n",
    "\n",
    "df = storage.pandas_from_json(r)\n",
    "#df = storage.pandas_from_csv(r)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T19:19:45.026456Z",
     "iopub.status.busy": "2021-07-05T19:19:45.026220Z",
     "iopub.status.idle": "2021-07-05T19:19:45.738111Z",
     "shell.execute_reply": "2021-07-05T19:19:45.737588Z",
     "shell.execute_reply.started": "2021-07-05T19:19:45.026434Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.9 ms, sys: 22.8 ms, total: 82.7 ms\n",
      "Wall time: 703 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created Dataframe with dimensions: (nrow, ncol) = (280932, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7752.117</td>\n",
       "      <td>11.874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7840.151</td>\n",
       "      <td>12.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7935.996</td>\n",
       "      <td>12.156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8039.684</td>\n",
       "      <td>12.315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8151.316</td>\n",
       "      <td>12.486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0       1\n",
       "0  7752.117  11.874\n",
       "1  7840.151  12.009\n",
       "2  7935.996  12.156\n",
       "3  8039.684  12.315\n",
       "4  8151.316  12.486"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r = s3.select_object_content(\n",
    "    Bucket=BUCKET,\n",
    "    Key='/blair-drummond/sql-example/TotalPopulation.csv.gz',\n",
    "    ExpressionType='SQL',\n",
    "    # Note, there's no ';' at the end.\n",
    "    Expression=\"\"\"\n",
    "    SELECT PopTotal,PopDensity FROM s3object s \n",
    "    \"\"\",\n",
    "    InputSerialization={\n",
    "        'CSV': {\n",
    "            # Use this if your CSV file has a header. Else set to \"NONE\".\n",
    "            \"FileHeaderInfo\": \"USE\",\n",
    "            'RecordDelimiter': '\\n',\n",
    "            'FieldDelimiter': ',',\n",
    "        },\n",
    "        # Remove this if the file isn't compressed.\n",
    "        'CompressionType': 'GZIP',\n",
    "    },\n",
    "    #OutputSerialization={'JSON': {}},\n",
    "    OutputSerialization={'CSV': {'RecordDelimiter': '\\n', 'FieldDelimiter': ','}},\n",
    ")\n",
    "\n",
    "#df = storage.pandas_from_json(r)\n",
    "df = storage.pandas_from_csv(r)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond Pandas: Arrow v.s. Pandas, CSV v.s. Parquet\n",
    "\n",
    "*Apache Arrow* is a newer tool designed for efficient data storage and retrieval. It's how Pandas opens `.parquet` files. We're going to do some benchmarking here, and we'll look at an experiment with the following variables.\n",
    "\n",
    "1. File Format\n",
    "    - `.csv` \n",
    "    - `.csv.gz` \n",
    "    - `.parquet`\n",
    "    \n",
    "2. Query type\n",
    "    - *Row Extraction*\n",
    "    - *Column Extraction*\n",
    "    \n",
    "3. Python Tool\n",
    "    - `pyarrow`\n",
    "    - `pandas`\n",
    "\n",
    "\n",
    "We're going to test every combination of these, to see how they work with eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T19:19:50.658578Z",
     "iopub.status.busy": "2021-07-05T19:19:50.658329Z",
     "iopub.status.idle": "2021-07-05T19:19:50.662941Z",
     "shell.execute_reply": "2021-07-05T19:19:50.662364Z",
     "shell.execute_reply.started": "2021-07-05T19:19:50.658553Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "###           Arrow versus Pandas            ### \n",
    "################################################\n",
    "import time\n",
    "def timing(f):\n",
    "    \"\"\" Discard the output of the function, but get the time. \"\"\"\n",
    "    def wrap(*args):\n",
    "        time1 = time.time()\n",
    "        f(*args)\n",
    "        time2 = time.time()\n",
    "        # milliseconds\n",
    "        ms = (time2-time1)*1000.0\n",
    "        return ms\n",
    "    return wrap\n",
    "\n",
    "funcs = {\n",
    "    'arrow'  : timing(storage.arrow_from_json),    \n",
    "    'pandas' : timing(storage.pandas_from_json)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T19:19:53.634323Z",
     "iopub.status.busy": "2021-07-05T19:19:53.634072Z",
     "iopub.status.idle": "2021-07-05T19:19:53.637396Z",
     "shell.execute_reply": "2021-07-05T19:19:53.636833Z",
     "shell.execute_reply.started": "2021-07-05T19:19:53.634299Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "###       Row versus Column Operations       ###\n",
    "################################################\n",
    "\n",
    "## Query is adjusted so that roughly \n",
    "## the same amount of data is scanned.\n",
    "sql = { \n",
    "        # nrow = 4420, ncol = 2;   nrow*ncol = 8840\n",
    "        'column' : \"\"\"\n",
    "        SELECT PopTotal,PopDensity FROM s3object s \n",
    "        LIMIT 4420\n",
    "        \"\"\",\n",
    "        # nrow = 884, ncol = 10;  nrow*ncol = 8840\n",
    "        'row' : \"\"\"\n",
    "        SELECT * FROM s3object s \n",
    "        WHERE s.Location like '%Canada%'\n",
    "        \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T19:20:03.785886Z",
     "iopub.status.busy": "2021-07-05T19:20:03.785627Z",
     "iopub.status.idle": "2021-07-05T19:20:03.790652Z",
     "shell.execute_reply": "2021-07-05T19:20:03.790103Z",
     "shell.execute_reply.started": "2021-07-05T19:20:03.785861Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "###                File Format               ###\n",
    "################################################\n",
    "\n",
    "## Note that csv.gz is smaller than parquet!\n",
    "\n",
    "def exp_csv(sql_query):\n",
    "    return s3.select_object_content(\n",
    "        Bucket=BUCKET,\n",
    "        # File size = 21 mb\n",
    "        Key='/blair-drummond/sql-example/TotalPopulation.csv',\n",
    "        ExpressionType='SQL',\n",
    "        Expression=sql_query,\n",
    "        InputSerialization={'CSV': {\"FileHeaderInfo\": \"USE\"}},\n",
    "        OutputSerialization={'JSON': {}}\n",
    "    )\n",
    "\n",
    "\n",
    "def exp_csv_gz(sql_query):\n",
    "    return s3.select_object_content(\n",
    "        Bucket=BUCKET,\n",
    "        # File size = 5.6 mb\n",
    "        Key='/blair-drummond/sql-example/TotalPopulation.csv.gz',\n",
    "        ExpressionType='SQL',\n",
    "        Expression=sql_query,\n",
    "        InputSerialization={\n",
    "            'CSV': {\"FileHeaderInfo\": \"USE\"},\n",
    "            'CompressionType': 'GZIP',\n",
    "        },\n",
    "        OutputSerialization={'JSON': {}}\n",
    "    )\n",
    "\n",
    "\n",
    "formats = {\n",
    "    'csv'     : exp_csv,\n",
    "    'csv.gz'  : exp_csv_gz, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the experiment!\n",
    "\n",
    "**Note: ** \n",
    "* Parquet is dsabled by default since hostile crafted input can easily crash the server\n",
    "* To enable Parquet set the environment variable MINIO_API_SELECT_PARQUET=on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T19:20:08.798723Z",
     "iopub.status.busy": "2021-07-05T19:20:08.798490Z",
     "iopub.status.idle": "2021-07-05T19:20:08.801467Z",
     "shell.execute_reply": "2021-07-05T19:20:08.800922Z",
     "shell.execute_reply.started": "2021-07-05T19:20:08.798700Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T19:20:15.110380Z",
     "iopub.status.busy": "2021-07-05T19:20:15.110121Z",
     "iopub.status.idle": "2021-07-05T19:20:15.410117Z",
     "shell.execute_reply": "2021-07-05T19:20:15.409612Z",
     "shell.execute_reply.started": "2021-07-05T19:20:15.110353Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created Dataframe with dimensions: (nrow, ncol) = (4420, 2)\n",
      "Created Dataframe with dimensions: (nrow, ncol) = (4420, 2)\n",
      "Created Dataframe with dimensions: (nrow, ncol) = (4420, 2)\n",
      "Created Dataframe with dimensions: (nrow, ncol) = (4420, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csv</th>\n",
       "      <th>csv.gz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pandas</th>\n",
       "      <td>22.651911</td>\n",
       "      <td>27.396202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrow</th>\n",
       "      <td>9.721279</td>\n",
       "      <td>12.241602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              csv     csv.gz\n",
       "pandas  22.651911  27.396202\n",
       "arrow    9.721279  12.241602"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### By Column\n",
    "\n",
    "col_exp = lambda backend,file: funcs[backend](formats[file](sql['column']))\n",
    "\n",
    "cols = pd.DataFrame({\n",
    "    'csv'     : [ col_exp('pandas', 'csv'),     col_exp('arrow', 'csv')      ],\n",
    "    'csv.gz'  : [ col_exp('pandas', 'csv.gz'),  col_exp('arrow', 'csv.gz')   ]\n",
    "    }, index=['pandas', 'arrow'])\n",
    "\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T19:20:25.633377Z",
     "iopub.status.busy": "2021-07-05T19:20:25.633139Z",
     "iopub.status.idle": "2021-07-05T19:20:28.473167Z",
     "shell.execute_reply": "2021-07-05T19:20:28.472623Z",
     "shell.execute_reply.started": "2021-07-05T19:20:25.633355Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created Dataframe with dimensions: (nrow, ncol) = (884, 10)\n",
      "Created Dataframe with dimensions: (nrow, ncol) = (884, 10)\n",
      "Created Dataframe with dimensions: (nrow, ncol) = (884, 10)\n",
      "Created Dataframe with dimensions: (nrow, ncol) = (884, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csv</th>\n",
       "      <th>csv.gz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pandas</th>\n",
       "      <td>614.407539</td>\n",
       "      <td>572.877884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrow</th>\n",
       "      <td>567.351580</td>\n",
       "      <td>504.571199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               csv      csv.gz\n",
       "pandas  614.407539  572.877884\n",
       "arrow   567.351580  504.571199"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### By Row\n",
    "\n",
    "row_exp = lambda backend,file: funcs[backend](formats[file](sql['row']))\n",
    "\n",
    "rows = pd.DataFrame({\n",
    "    'csv'     : [ row_exp('pandas', 'csv'),     row_exp('arrow', 'csv')      ],\n",
    "    'csv.gz'  : [ row_exp('pandas', 'csv.gz'),  row_exp('arrow', 'csv.gz')   ]\n",
    "    }, index=['pandas', 'arrow'])\n",
    "\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: I think Parquet will probably perform much better as the file size increases. Our files here are pretty small.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: arrow > pandas.\n",
    "\n",
    "This experiment done with a very small dataset, but there are two observations:\n",
    "\n",
    "1. **Arrow is faster than pandas in every case**.\n",
    "2. **Scanning columns is WAY faster than scanning Rows**.\n",
    "\n",
    "Also, note that while `csv.gz` is slightly slower than `csv`, the `csv.gz` files are `1/4` the size in storage. For large files, this will translate to faster transfer speeds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
