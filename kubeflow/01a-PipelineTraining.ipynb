{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Santander Customer Transaction Prediction\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![image](imgs/sant_comp.png)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problem and data\n",
    "- Identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Anonymized dataset containing `numeric feature variables`, the binary `target column`."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The task is to predict the value of binary `target` column in the test set."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# End-to-end real world ML pipeline\n",
    "![image](imgs/demo_pipeline.png)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kubeflow Pipelines"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Kubeflow Pipelines is a platform for building and deploying portable, scalable machine learning (ML) workflows based on docker containers.\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Kubeflow pipelines make it easy to implement production grade machine learning workflows without worrying about the low-level details of managing a Kubernetes cluster."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kubeflow Pipelines Goals"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![image](imgs/kf_goals1.png)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " - Visual depiction of pipeline topology"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For any given step you can use libraries and tools of choice"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A single step cloud be a single node instance GPU or CPU / or something that can run on a distributed fashion."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Easily convert a dockerized task into a pipeline step by following specific format of how the docker container accepts input and produces output.\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Python SDK to specify the sequence of steps of your workflow"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ML Workflow Orchestration\n",
    "\n",
    "![image](imgs/example_pipeline1.png)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kubeflow Pipelines Goals\n",
    "![image](imgs/kf_goals2.png)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Package the define pipeline up as a zip file"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Even an individual step can be packaged up as a reusable component."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kubeflow Pipelines Goals\n",
    "![image](imgs/kf_goals3.png)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Rapidly iterate on your ideas in a reliable and manageable way"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Historical view of any prior runs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Filters to find past runs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Clone a past run"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Comparison feature"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What Constitutes a Kubeflow Pipeline?"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 1. Containerized implementations of ML tasks\n",
    "- Docker containers provide portability, repeatability and encapsulation\n",
    "- Any tools or libraries could be adopted inside your containerized docker image"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What Constitutes a Kubeflow Pipeline?\n",
    "### 2. Specification of the sequence of steps\n",
    "- Specification of the sequence of steps that should run, how the data flows between these steps and how to connect the output of one step with the inputs of downstream step\n",
    "- Model multi-step workflows as a sequence of steps or capture the dependencies between tasks using a graph (DAG)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What Constitutes a Kubeflow Pipeline?\n",
    "### 3. Input Parameters\n",
    "- Defining input parameters that will be exposed to the end user in a UI form."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Santander Customer Transaction Prediction: Model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "(train_input_fn, feature_names, feature_columns) = \\\n",
    "    make_inputs_from_np_arrays(features_np=train_data[:, 1:],\n",
    "                               label_np=train_data[:, 0:1])\n",
    "classifier = tf.estimator.BoostedTreesClassifier(\n",
    "    feature_columns,\n",
    "    n_batches_per_layer=args.num_batch,\n",
    "    model_dir=args.job_dir,\n",
    "    n_trees=args.n_trees,\n",
    "    max_depth=args.max_depth,\n",
    "    learning_rate=args.learning_rate,\n",
    "    )\n",
    "classifier.train(train_input_fn)\n",
    "```"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Setup"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "EXP_NAME='Santander Training Pipeline'\n",
    "OUTPUT_DIR='s3://kubeflow-pipelines-demo/tfx'\n",
    "\n",
    "PROJECT_NAME = 'kf-pipelines'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!pip install kfp --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Create an Experiment in the Kubeflow pipeline"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Kubeflow Pipeline requires having an _experiment_ before making a run. An experiment is a group of comparable runs."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import compiler\n",
    "import kfp.dsl as dsl\n",
    "\n",
    "import kfp.notebook\n",
    "import kfp.gcp as gcp\n",
    "\n",
    "client = kfp.Client()\n",
    "exp = client.create_experiment(name=EXP_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Define a Pipeline"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Authoring a pipeline is just like authoring a normal Python function with a sepcial decoration for the SDK to recognize it."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The pipeline function describes the topology of the pipeline and how the input of one component is passed down the stream as an output to another component. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In the below pipeline, all the docker container images referenced in the pipeline are already built"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Imports"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "import kfp\n",
    "from kfp import components\n",
    "from kfp import dsl\n",
    "from kfp import azure\n",
    "from kfp import onprem\n",
    "import os\n",
    "\n",
    "platform = 'Azure'\n",
    "\n",
    "minio_endpoint = os.environ.get('MINIO_URL', 'minio-service.kubeflow.svc.cluster.local:9000')\n",
    "minio_key = os.environ.get('MINIO_KEY', 'minio')\n",
    "minio_secret = os.environ.get('MINIO_SECRET', 'XXXXXX')\n",
    "\n",
    "print('Minio parameters : URL ', minio_endpoint, ' key ', minio_key, ' secret ', minio_secret)\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = minio_key\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = minio_secret\n",
    "os.environ['AWS_REGION'] = 'us-west-1'\n",
    "os.environ['S3_REGION'] = 'us-west-1'\n",
    "os.environ['S3_ENDPOINT'] = minio_endpoint\n",
    "os.environ['S3_USE_HTTPS'] = '0'\n",
    "os.environ['S3_VERIFY_SSL'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convert a dockerized task into a pipeline step"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Define a python function wrapper for that container by invoking an sdk function ContainerOp\n",
    "\n",
    "```python\n",
    "def dataproc_train_op(\n",
    "    project,\n",
    "    train_data,\n",
    "    eval_data,\n",
    "    output\n",
    "):\n",
    "    return dsl.ContainerOp(\n",
    "        name='Dataproc - Train XGBoost model',\n",
    "        image='gcr.io/ml-pipeline/xgboost-training:v1.1',\n",
    "        arguments=[\n",
    "            '--project', project,\n",
    "            '--train', train_data,\n",
    "            '--eval', eval_data,\n",
    "            '--output', output,\n",
    "        ],\n",
    "        file_outputs={\n",
    "            'output': '/output.txt',\n",
    "        }\n",
    "    )\n",
    "```"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Define a docker container specification file\n",
    "```yaml\n",
    "name: Train Boosted Trees\n",
    "description: Trains Boosted Trees using Tensorflow\n",
    "inputs:\n",
    "  - {name: Transformed data dir,  type: GCSPath, description: 'Path to transformed data'}\n",
    "  - {name: Learning rate,type: Float, default: '0.1', description: 'Learning rate for training.'}\n",
    "outputs:\n",
    "  - {name: Training output dir, type: GCSPath, description: 'GCS or local directory.'} \n",
    "implementation:\n",
    "  container:\n",
    "    image: us.gcr.io/kf-pipelines/kubeflow-train_boosted:v0.3\n",
    "    command: [python, -m, trainer.boosted]\n",
    "    args: [\n",
    "      --transformed-data-dir, {inputValue: Transformed data dir},\n",
    "      --learning-rate, {inputValue: Learning rate},\n",
    "      --steps, {inputValue: Steps},\n",
    "    ]\n",
    "    fileOutputs:\n",
    "      Training output dir: /output.txt\n",
    "```"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Load components (steps) from specification YAML file"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dataflow_tf_transform_op = \\\n",
    "    components.load_component_from_file('pipeline_steps/preprocessing/tft/component.yaml')\n",
    "tf_train_op = \\\n",
    "    components.load_component_from_file('pipeline_steps/training/dnntrainer/component.yaml')\n",
    "dataflow_tf_predict_op = \\\n",
    "    components.load_component_from_file('pipeline_steps/training/predict/component.yaml')\n",
    "confusion_matrix_op = \\\n",
    "    components.load_component_from_file('pipeline_steps/metrics/confusion_matrix/component.yaml')\n",
    "roc_op = \\\n",
    "    components.load_component_from_file('pipeline_steps/metrics/roc/component.yaml')\n",
    "kubeflow_deploy_op = \\\n",
    "    components.load_component_from_file('pipeline_steps/serving/deployer/component.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Define pipeline function"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from kubernetes import client as k8s_client\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Santander Customer Transaction Prediction',\n",
    "    description='Example pipeline that does classification with model analysis based on Santander customer transaction dataset.'\n",
    ")\n",
    "def santander_transaction_classification(\n",
    "        output,\n",
    "        project,\n",
    "        train='s3://kubeflow-pipelines-demo/dataset/train.csv',\n",
    "        evaluation='s3://kubeflow-pipelines-demo/dataset/test.csv',\n",
    "        mode='local',\n",
    "        preprocess_module='s3://kubeflow-pipelines-demo/dataset/preprocessing.py',\n",
    "        learning_rate=0.1,\n",
    "        hidden_layer_size='1500',\n",
    "        steps=3000\n",
    "):\n",
    "    output_template = str(output) + '/{{workflow.uid}}/{{pod.name}}/data'\n",
    "    target_class_lambda = \"\"\"lambda x: x['target']\"\"\"\n",
    "\n",
    "    tf_server_name = 'kfdemo-service'\n",
    "\n",
    "    if platform != 'Azure':\n",
    "        vop = dsl.VolumeOp(\n",
    "            name=\"create_pvc\",\n",
    "            resource_name=\"pipeline-pvc\",\n",
    "            modes=dsl.VOLUME_MODE_RWO,\n",
    "            size=\"1Gi\"\n",
    "        )\n",
    "\n",
    "        checkout = dsl.ContainerOp(\n",
    "            name=\"checkout\",\n",
    "            image=\"alpine/git:latest\",\n",
    "            command=[\"git\", \"clone\", \"https://github.com/kubeflow/pipelines.git\", str(output) + \"/pipelines\"],\n",
    "        ).apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\n",
    "        checkout.after(vop)\n",
    "\n",
    "    preprocess = dataflow_tf_transform_op(\n",
    "        training_data_file_pattern=train,\n",
    "        evaluation_data_file_pattern=evaluation,\n",
    "        schema='s3://kubeflow-pipelines-demo/dataset/not.txt',\n",
    "        gcp_project=project,\n",
    "        run_mode=mode,\n",
    "        preprocessing_module=preprocess_module,\n",
    "        transformed_data_dir=output_template\n",
    "    )\n",
    "\n",
    "    training = tf_train_op(\n",
    "        transformed_data_dir=preprocess.output,\n",
    "        schema='s3://kubeflow-pipelines-demo/dataset/not.txt',\n",
    "        learning_rate=learning_rate,\n",
    "        hidden_layer_size=hidden_layer_size,\n",
    "        steps=steps,\n",
    "        target='tips',\n",
    "        preprocessing_module=preprocess_module,\n",
    "        training_output_dir=output_template\n",
    "    )\n",
    "\n",
    "    prediction = dataflow_tf_predict_op(\n",
    "        data_file_pattern=evaluation,\n",
    "        schema='s3://kubeflow-pipelines-demo/dataset/not.txt',\n",
    "        target_column='tips',\n",
    "        model=training.outputs['training_output_dir'],\n",
    "        run_mode=mode,\n",
    "        gcp_project=project,\n",
    "        predictions_dir=output_template\n",
    "    )\n",
    "\n",
    "    cm = confusion_matrix_op(\n",
    "        predictions=prediction.outputs['predictions_dir'],\n",
    "        output_dir=output_template\n",
    "    )\n",
    "\n",
    "    roc = roc_op(\n",
    "        predictions_dir=prediction.outputs['predictions_dir'],\n",
    "        target_lambda=target_class_lambda,\n",
    "        output_dir=output_template\n",
    "    )\n",
    "\n",
    "    steps = [training, prediction, cm, roc]\n",
    "    for step in steps:\n",
    "        if platform == 'GCP':\n",
    "            step.apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "        elif platform != 'Azure':\n",
    "            step.apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\n",
    "\n",
    "    dsl.get_pipeline_conf().set_image_pull_secrets([k8s_client.V1ObjectReference(name=\"k8scc01covidacr-registry-connection\")])\n",
    "    dsl.get_pipeline_conf().add_op_transformer(lambda cop: cop.container.add_env_variable(k8s_client.V1EnvVar(name='AWS_ACCESS_KEY_ID', value=os.environ['AWS_ACCESS_KEY_ID'])))\n",
    "    dsl.get_pipeline_conf().add_op_transformer(lambda cop: cop.container.add_env_variable(k8s_client.V1EnvVar(name='AWS_SECRET_ACCESS_KEY', value=os.environ['AWS_SECRET_ACCESS_KEY'])))\n",
    "    dsl.get_pipeline_conf().add_op_transformer(lambda cop: cop.container.add_env_variable(k8s_client.V1EnvVar(name='AWS_REGION', value=os.environ['AWS_REGION'])))\n",
    "    dsl.get_pipeline_conf().add_op_transformer(lambda cop: cop.container.add_env_variable(k8s_client.V1EnvVar(name='S3_REGION', value=os.environ['S3_REGION'])))\n",
    "    dsl.get_pipeline_conf().add_op_transformer(lambda cop: cop.container.add_env_variable(k8s_client.V1EnvVar(name='S3_ENDPOINT', value=os.environ['S3_ENDPOINT'])))\n",
    "    dsl.get_pipeline_conf().add_op_transformer(lambda cop: cop.container.add_env_variable(k8s_client.V1EnvVar(name='S3_USE_HTTPS', value=os.environ['S3_USE_HTTPS'])))\n",
    "    dsl.get_pipeline_conf().add_op_transformer(lambda cop: cop.container.add_env_variable(k8s_client.V1EnvVar(name='S3_VERIFY_SSL', value=os.environ['S3_VERIFY_SSL'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Compile the pipeline"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Compile the pipeline into a tar package"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(santander_transaction_classification, 'santander_training_pipeline.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Submit the run with parameters"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "run = client.run_pipeline(exp.id, 'santander_training_pipeline-' + time.strftime(\"%Y%m%d-%H%M%S\"), 'santander_training_pipeline.zip',\n",
    "                          params={'output': 's3://kubeflow-pipelines-demo/tfx', 'project': 'kf-pipelines',\n",
    "                                  'train': 's3://kubeflow-pipelines-demo/dataset/train.csv',\n",
    "                                  'evaluation': 's3://kubeflow-pipelines-demo/dataset/test-min.csv', 'mode': 'local',\n",
    "                                  'preprocess_module': 's3://kubeflow-pipelines-demo/dataset/preprocessing.py',\n",
    "                                  'learning_rate': 0.1, 'hidden_layer_size': '1500', 'steps': 3000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}