{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "\n",
    "**Tutorial Difficulty: Beginner**\n",
    "\n",
    "This notebook documents:\n",
    "* How to define a simple pipeline defined by operations that are contained in docker containers\n",
    "* An example of a pipeline where each step communicates its outputs to the next step by writing them to a file locally and having kubeflow pipelines transfer results between steps as JSON strings\n",
    "* A map-reduce workflow pattern, where we:\n",
    "    * break our work into many small pieces that can be done in parallel (map), and then\n",
    "    * aggregate the product of that work back to some final result (reduce)\n",
    "\n",
    "To bring it all together, we apply these techniques to compute an estimate of pi. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline to estimate pi, in the most ridiculously parallel way possible\n",
    "\n",
    "This pipeline estimates pi by repeating the process of:\n",
    "\n",
    "* Picking a random location inside a 2x2 square centered on the origin\n",
    "* Checking whether or not that point also resides inside a unit circle centered on the origin\n",
    "* Assigning a value to this point:\n",
    "    * value = 1 if the point is inside the circle (red)\n",
    "    * value = 0 if the point is outside the circle (blue)\n",
    "\n",
    "By doing this repeatedly and taking 4x the average value over all repetitions, we obtain an estimate of pi\n",
    "\n",
    "![Parallel Monte Carlo](images/Pi.png)\n",
    "\n",
    "We implement this procedure using the map-reduce pattern by:\n",
    "* **Map:** Generating N **sample** operations which pick the point and assign it a value of 0/1.  Note that each **sample** operation is given a different random seed to ensure it picks a different point in the square\n",
    "* **Reduce:** Combining all **sample** results in an **average** step which then returns the estimate of pi\n",
    "\n",
    "The pipeline, as visualized in kubeflow pipelines, looks like this:\n",
    "\n",
    "![The pipeline](images/kf-pipeline.png)\n",
    "\n",
    "Where the top row of **sample** operations all feed to the single **average** step on the second row.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up our Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from utilities import validate_kfp_name, validate_bucket_name\n",
    "\n",
    "#################################\n",
    "### Configure your variables ####\n",
    "#################################\n",
    "\n",
    "# Number of parallel sample steps we run\n",
    "SAMPLES = 15\n",
    "\n",
    "# Name of our experiment in kubeflow\n",
    "# Experiment name can contain alphanumeric characters, hyphens, or underscores\n",
    "EXPERIMENT_NAME = \"compute-pi\"\n",
    "assert validate_kfp_name(EXPERIMENT_NAME)\n",
    "\n",
    "# Names and container images for our pipeline operations\n",
    "# Pipeline operations have the same name restrictions as experiments\n",
    "SAMPLE_IMAGE_PATH = f\"k8scc01covidacr.azurecr.io/blair-kf-pipeline-pi-sample:v10\"\n",
    "SAMPLE_PIPELINE_OP_NAME = \"one-pi-estimate\"\n",
    "assert validate_kfp_name(SAMPLE_PIPELINE_OP_NAME)\n",
    "\n",
    "AVERAGE_IMAGE_PATH = f\"k8scc01covidacr.azurecr.io/blair-kf-pipeline-pi-average:v10\"\n",
    "AVERAGE_PIPELINE_OP_NAME = \"aggregate-pi-estimate\"\n",
    "assert validate_kfp_name(AVERAGE_PIPELINE_OP_NAME)\n",
    "\n",
    "########################################\n",
    "### This gets fed into the map step ####\n",
    "########################################\n",
    "# Define a generator that creates the numeric random seed for each sample step\n",
    "def seeds(how_many=SAMPLES):\n",
    "    \"\"\" Define the seeds for the algorithms \"\"\"\n",
    "    for i in range(how_many):\n",
    "        yield { \"seed\" : 3 * i }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we define all operations in our pipeline, as well as how they chain together.  Pipelines are defined by separate, typically single purpose, operations (or steps).  Each pipeline operation could be used once, multiple times, etc., and might depend on results from upstream steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the pipeline operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pipeline here has two steps, both of which are defined in docker containers (paths to those containers were specified above and are used below).  Each step is a factory function that returns ContainerOp's.  These ContainerOps are then used to define the actual pipeline next.\n",
    "\n",
    "For this example, the containers are already built and pushed to ```k8scc01covidacr.azurecr.io``` (see ```SAMPLE_IMAGE_PATH``` and ```AVERAGE_IMAGE_PATH``` above).  Each container has a small shell script to do the work for that operation (check out the scripts at ```./sample/sample.sh``` and ```./average/average.sh``` to see how they work).  This notebook defines two kubeflow pipeline operation (```sample_op``` and ```average_op```) that specify how kubeflow interacts with those containers (how to call them, what args to provide, what to do with their outputs, ...).  \n",
    "\n",
    "For this example, we pass arguments like our random seed as JSON strings rather than bare numbers.  It isn't really necessary here, but gives an example of how one can pass a non-trivial data type.  This can be extended to complicated dictionaries with many keys and even serialized (JSONified) versions of objects like dataframes.  \n",
    "\n",
    "Side note: Technically ```sample_op``` and ```average_op``` are factories that return ContainerOp instances, and kubeflow pipelines uses those ContainerOp instances to build a pipeline, but if none of that makes sense its ok..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "import itertools\n",
    "\n",
    "def sample_op(params):\n",
    "    \"\"\"\n",
    "    Factory for \"sample\" pipeline operation\n",
    "    \n",
    "    Operations created by this factory invoke the SAMPLE step by invoking\n",
    "    a docker container that includes ./sample/sample.sh.  sample.sh accepts\n",
    "    a random seed.\n",
    "    \n",
    "    The result of this operation will be an out.json file with contents:\n",
    "        { \"x\" : point_x_coord, \"y\" : point_y_coord, \"result\" : 0_or_1 }\n",
    "    This result will be passed back as _sample_op_result.output\n",
    "    \n",
    "    Args:\n",
    "        params (str): JSON string of a dict that has a seed value as key \"seed\", eg:\n",
    "                        '{\"seed\": 5, \"some_ignored_key\": \"value_doesnt_matter\"}\n",
    "\n",
    "    Returns:\n",
    "        JSON string of out.json\n",
    "    \"\"\"\n",
    "    # Assemble arguments passed to the script called by the container op\n",
    "    arguments = [\"--params\", params]\n",
    "    \n",
    "    # And to the ContainerOp constructor\n",
    "    containerop_kwargs = dict(\n",
    "        name=SAMPLE_PIPELINE_OP_NAME,\n",
    "        image=f'{SAMPLE_IMAGE_PATH}',\n",
    "        arguments=arguments,\n",
    "        # Specify where kubeflow will get output from\n",
    "        file_outputs={'data': \"./output/out.json\"},\n",
    "    )\n",
    "    \n",
    "    # Return the actual Container Op, with additional memory and cpu constraints\n",
    "    return dsl.ContainerOp(\n",
    "        **containerop_kwargs,\n",
    "    ).set_memory_request(\n",
    "        \"100M\"\n",
    "    ).set_memory_limit(\n",
    "        \"150M\"\n",
    "    ).set_cpu_request(\n",
    "        \"0.1\"\n",
    "    ).set_cpu_limit(\n",
    "        \"1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_op(jsons=None):\n",
    "    \"\"\"\n",
    "    Factory for \"average\" pipeline operation\n",
    "        \n",
    "    Operations created by this factory invoke the AVERAGE step by invoking\n",
    "    a docker container that includes ./average/average.sh.  average.sh accepts\n",
    "    results from one or more SAMPLE steps as JSON strings and returns output\n",
    "    as a JSON string\n",
    "\n",
    "    Generates an output file out.json with contents:\n",
    "        { \"pi\": estimate_of_pi, \"samples\": number_of_samples }\n",
    "    This result is passed back as _sample_op_result.output\n",
    "    \n",
    "    Args:\n",
    "        jsons (list): List of JSON strings of output from one or more \n",
    "                      \"sample\" steps.  Each must have a \"result\" key with the value\n",
    "                      from the sample step.\n",
    "\n",
    "    Returns:\n",
    "        JSON result file is captured by Kubeflow and saved as an artifact\n",
    "    \"\"\"\n",
    "    if not jsons:\n",
    "        raise ValueError(\"Must specify one or more json string inputs\")\n",
    "    \n",
    "    # Assemble arguments passed to the script called by the container op\n",
    "    arguments = []\n",
    "    if jsons:\n",
    "        json_args = list(itertools.chain.from_iterable([(\"--json\", j) for j in jsons]))\n",
    "        arguments += json_args\n",
    "    \n",
    "    # And to the ContainerOp constructor\n",
    "    containerop_kwargs = dict(\n",
    "        name=AVERAGE_PIPELINE_OP_NAME,\n",
    "        image=f'{AVERAGE_IMAGE_PATH}',\n",
    "        arguments=arguments,\n",
    "        # Specify where to get output from\n",
    "        file_outputs={'data': \"./output/out.json\"},\n",
    "    )\n",
    "    \n",
    "    # Return the actual Container Op\n",
    "    return dsl.ContainerOp(**containerop_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the pipeline here as a python function wrapped in the @dsl.pipeline decorator.  This function, in our case compute_pi(), defines the logic for how all the steps within the pipeline chain together.  In our case, it tells kubeflow pipelines to run N **sample** operations in parallel, and run a single **average** operation that consumes output from all the **sample** operations.  \n",
    "\n",
    "This dependency of **average** on **sample**s is what lets kfp know the order in which to run things.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "### You can change below this      ###\n",
    "### Create the pipeline            ###\n",
    "######################################\n",
    "@dsl.pipeline(\n",
    "    name=\"Estimate Pi\",\n",
    "    description='Estimate Pi using a Map-Reduce pattern'\n",
    ")\n",
    "def compute_pi():\n",
    "    \"\"\"Compute Pi\"\"\"\n",
    "\n",
    "    # Create arguments for each \"sample\" operation in the pipeline\n",
    "    # Each operation gets its own seed and a path in minio to store its output\n",
    "    # params is passed as a json string with just {'seed': value_of_seed}\n",
    "    sample_args = [{'params': json.dumps(param)} for (i, param) in enumerate(seeds())]\n",
    "\n",
    "    # Create a sample operation for each arg\n",
    "    sample_ops = [sample_op(**kwarg) for kwarg in sample_args]\n",
    "\n",
    "    # Define the average operation which consumes output from all the sample_ops\n",
    "    _average_op = average_op(\n",
    "        jsons=[s.output for s in sample_ops],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to understand here that while ```compute_pi``` describes the pipeline in python code, most of the computation is not done when we run the above block.  Calling ```sample_op``` does not do a **sample** operation, it creates a ContainerOp that tells kubeflow pipelines to run a **sample** operation when running the pipeline.  And when we do something like:\n",
    "```\n",
    "_average_op = average_op(\n",
    "    jsons=[s.output for s in sample_ops],\n",
    ")\n",
    "```\n",
    "\n",
    "```s.output``` is not the actual output of a **sample** operation, it is a placeholder that tells kubeflow pipelines \"when you get to this part in the pipeline, insert the output that you've previous computed for this **sample** operation here\".  This way you can pipe data from one pipeline step to the next without having to actually compute it now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we translate our compute_pi function into a zipped yaml definition of the pipeline.  This zip file is how we tell kubeflow pipelines exactly what to run for your pipeline.  Download and take a look inside to get a better understanding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported pipeline definition to compute-pi.zip\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "### DON'T EDIT:                             ###\n",
    "### Create the pipeline description for kfp ###\n",
    "###############################################\n",
    "from kfp import compiler\n",
    "experiment_yaml_zip = EXPERIMENT_NAME + '.zip'\n",
    "compiler.Compiler().compile(\n",
    "    compute_pi,\n",
    "    experiment_yaml_zip\n",
    ")\n",
    "print(f\"Exported pipeline definition to {experiment_yaml_zip}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ready to roll! Let's run this pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/9b336825-fd42-4535-a5a4-1d11a171c833\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###################################\n",
    "### DON'T EDIT:                 ###\n",
    "### Create the Experiment       ###\n",
    "###################################\n",
    "import kfp\n",
    "client = kfp.Client()\n",
    "exp = client.create_experiment(name=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/67b1d885-7916-42b2-808d-d5cab4ced15d\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###############################################\n",
    "### DON'T EDIT:                             ###\n",
    "### Run the pipeline                        ###\n",
    "###############################################\n",
    "import time\n",
    "run = client.run_pipeline(\n",
    "    exp.id,\n",
    "    EXPERIMENT_NAME + '-' + time.strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    EXPERIMENT_NAME + '.zip',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the pipeline running, click the link above.  To access the JSON string returned by the Average step, click on that step in the pipeline and look in the output artifact as shown below.\n",
    "\n",
    "![pipeline with results](images/kf-pipeline_with_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this method of returning a result is likely not that useful for most problems.  See other other demos for saving results to minio or other locations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
