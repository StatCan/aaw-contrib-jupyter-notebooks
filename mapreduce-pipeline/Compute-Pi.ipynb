{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "\n",
    "**Tutorial Difficulty: Beginner**\n",
    "\n",
    "This notebook documents:\n",
    "* How to build a simple pipeline from operations defined with docker containers\n",
    "* A map-reduce workflow pattern, where we:\n",
    "    * (map) break our work into many small pieces that can be done in parallel, and then\n",
    "    * (reduce) aggregate the product of that work back to some final result\n",
    "\n",
    "To bring it all together, we apply these techniques to compute an estimate of pi. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline to estimate pi, in the most ridiculously parallel way possible\n",
    "\n",
    "This pipeline estimates pi by repeating the process of:\n",
    "\n",
    "* Picking a random location inside a 2x2 square centered on the origin\n",
    "* Checking whether or not that point also resides inside a unit circle centered on the origin\n",
    "* Assigning a value to this point:\n",
    "    * value = 4 if the point is inside the circle (red)\n",
    "    * value = 0 if the point is outside the circle (blue)\n",
    "\n",
    "By doing this repeatedly and taking the average value over all repetitions, we obtain an estimate of pi\n",
    "\n",
    "![Parallel Monte Carlo](images/Pi.png)\n",
    "\n",
    "We implement this procedure using the map-reduce pattern by:\n",
    "* **Map:** Generating N **sample** operations which pick the point and assign it a value of 0/4.  Note that each **sample** operation is given a different random seed to ensure it picks a different point in the square\n",
    "* **Reduce:** Combining all **sample** results in an **average** step which then returns the estimate of pi\n",
    "\n",
    "The pipeline, as visualized in kubeflow pipelines, looks like this:\n",
    "\n",
    "![The pipeline](images/kf-pipeline.png)\n",
    "\n",
    "Where the top row of **sample** operations all feed to the single **average** step on the second row.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up our Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define user-level project variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "### Configure your variables ####\n",
    "#################################\n",
    "\n",
    "# Number of parallel sample steps we run\n",
    "SAMPLES = 15\n",
    "\n",
    "# Name of our experiment in kubeflow\n",
    "# Experiment name can contain alphanumeric characters, hyphens, or underscores\n",
    "EXPERIMENT_NAME = \"compute-pi\"\n",
    "\n",
    "# Names we assign to our components (they're used in the definitions below)\n",
    "# This is what will show up in the Kubeflow Pipelines UI\n",
    "# These have the same naming restrictions as experiment names\n",
    "SAMPLE_PIPELINE_OP_NAME = \"sample\"\n",
    "AVERAGE_PIPELINE_OP_NAME = \"average\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### DON'T EDIT:         ###\n",
    "### Validate our inputs ###\n",
    "###########################\n",
    "from utilities import validate_kfp_name, validate_bucket_name\n",
    "\n",
    "assert validate_kfp_name(EXPERIMENT_NAME)\n",
    "assert validate_kfp_name(SAMPLE_PIPELINE_OP_NAME)\n",
    "assert validate_kfp_name(AVERAGE_PIPELINE_OP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "### DON'T EDIT:                      ###\n",
    "### Path to the containers used here ###\n",
    "########################################\n",
    "SAMPLE_IMAGE_PATH = f\"k8scc01covidacr.azurecr.io/kfp-components/map-reduce/sample:v1\"\n",
    "AVERAGE_IMAGE_PATH = f\"k8scc01covidacr.azurecr.io/kfp-components/map-reduce/average:v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we define all operations in our pipeline, as well as how they chain together.  Pipelines are defined by separate, typically single purpose, operations (or steps).  Each pipeline operation could be used once, multiple times, etc., and might depend on results from upstream steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the pipeline operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pipeline here has two steps, both of which are defined in docker containers (paths to those containers were specified above and are used below).  Each step is a factory function that returns ContainerOp's.  These ContainerOps are then used to define the actual pipeline next.\n",
    "\n",
    "For this example, the containers are already built and pushed to ```k8scc01covidacr.azurecr.io``` (see ```SAMPLE_IMAGE_PATH``` and ```AVERAGE_IMAGE_PATH``` above).  Each container has a small shell script to do the work for that operation (check out the scripts at ```./sample/sample.sh``` and ```./average/average.sh``` to see how they work).  This notebook defines two kubeflow pipeline operations (```sample_op``` and ```average_op```) that specify how kubeflow interacts with those containers (how to call them, what args to provide, what to do with their outputs, ...).  \n",
    "\n",
    "Side note: Technically ```sample_op``` and ```average_op``` are factories that return ContainerOp instances.  Kubeflow Pipelines uses those ContainerOp instances to construct its definition of your pipeline, but if none of that makes sense its ok..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "import itertools\n",
    "\n",
    "def sample_op(seed):\n",
    "    \"\"\"\n",
    "    Factory for \"sample\" pipeline operation\n",
    "    \n",
    "    Operations created by this factory invoke the SAMPLE step by invoking\n",
    "    a docker container that includes ./sample/sample.py.  sample.py accepts\n",
    "    a random seed as argument:\n",
    "    \n",
    "        sample.py SEED\n",
    "    \n",
    "    The result of this operation will be:\n",
    "        output_result.txt: A file with either 0 (outside the unit circle) or 4\n",
    "                           (inside the unit circle)\n",
    "        output_coordinate.txt: The (x, y) coordinate sampled here\n",
    "        input_seed.txt: A record of the seed used\n",
    "\n",
    "    These results are passed back in the .outputs in the ContainerOp result\n",
    "    \n",
    "    Args:\n",
    "        seed (number): Number used as a seed\n",
    "\n",
    "    Returns:\n",
    "        ContainerOp\n",
    "    \"\"\"\n",
    "    # Return the ContainerOp that defines our interaction with the container\n",
    "    op = dsl.ContainerOp(\n",
    "        name=SAMPLE_PIPELINE_OP_NAME,\n",
    "        image=SAMPLE_IMAGE_PATH,\n",
    "        arguments=[seed],\n",
    "        # Specify where kubeflow will get output from\n",
    "        file_outputs={\"result\": \"./output_result.txt\",\n",
    "                      \"coordinate\": \"./output_coordinate.txt\",\n",
    "                      \"seed\": \"./input_seed.txt\"\n",
    "                     },\n",
    "    )\n",
    "    \n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_op(numbers: list):\n",
    "    \"\"\"\n",
    "    Factory for \"average\" pipeline operation\n",
    "        \n",
    "    Operations created by this factory invoke the AVERAGE step by invoking\n",
    "    a docker container that includes ./average/average.py.  average.py accepts\n",
    "    one or more numbers as command line arguments and computes their average\n",
    "\n",
    "    The result of this script is:\n",
    "        out.txt: A file containing the average of the inputs\n",
    "\n",
    "    This result is passed back as .output in the ContainerOp result\n",
    "    \n",
    "    Args:\n",
    "        numbers (list): List of numeric results from one or more sample steps\n",
    "\n",
    "    Returns:\n",
    "        ContainerOp\n",
    "    \"\"\"\n",
    "    if len(numbers) == 0:\n",
    "        raise ValueError(\"numbers must be at least of length 1\")\n",
    "    \n",
    "    # Return the ContainerOp that defines our interaction with the container\n",
    "    op = dsl.ContainerOp(\n",
    "        name=AVERAGE_PIPELINE_OP_NAME,\n",
    "        image=AVERAGE_IMAGE_PATH,\n",
    "        arguments=numbers,\n",
    "        # Specify where to get output from\n",
    "        file_outputs={'result': \"./out.txt\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the pipeline here as a python function wrapped in the @dsl.pipeline decorator.  This function, in our case `compute_pi()`, defines the logic for how all the steps within the pipeline chain together.  `compute_pi` tells kubeflow pipelines to run N **sample** operations in parallel, and run a single **average** operation that consumes output (`sample_op.outputs['result']`) from all the **sample** operations.  \n",
    "\n",
    "This dependency of **average** on **sample**s is what lets kfp know the order in which to run things.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "### You can change below this      ###\n",
    "### Create the pipeline            ###\n",
    "######################################\n",
    "@dsl.pipeline(\n",
    "    name=\"Estimate Pi\",\n",
    "    description='Estimate Pi using a Map-Reduce pattern'\n",
    ")\n",
    "def compute_pi():\n",
    "    \"\"\"Compute Pi\"\"\"\n",
    "\n",
    "    # Create the seeds for our random samples\n",
    "    seeds = range(SAMPLES)\n",
    "    \n",
    "    # Create a \"sample\" operation for each seed passed to the pipeline\n",
    "    sample_ops = [sample_op(seed) for seed in seeds]\n",
    "\n",
    "    # Define the average operation which consumes the result from each of the sample_ops\n",
    "    # Note that the results for each sample_op, read in from their respective \n",
    "    # output_result.txt files, are available from the sample_op instances through\n",
    "    # the .outputs attribute\n",
    "    _average_op = average_op([s.outputs['result'] for s in sample_ops])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to understand here that while ```compute_pi``` describes the pipeline in python code, most of the computation is not done when we run the above block.  Calling ```sample_op``` does not do a **sample** operation, it creates a ContainerOp that tells kubeflow pipelines to run a **sample** operation when running the pipeline.  And when we do something like:\n",
    "```\n",
    "    _average_op = average_op([s.outputs['result'] for s in sample_ops])\n",
    "```\n",
    "\n",
    "```s.outputs['result']``` is not the actual output of a **sample** operation, it is a placeholder that tells kubeflow pipelines \"when you get to this part in the pipeline, insert the output that you've previous computed for this **sample** operation here\".  This way you can pipe data from one pipeline step to the next without having to actually compute it now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we translate our compute_pi function into a zipped yaml definition of the pipeline.  This zip file is how we tell kubeflow pipelines exactly what to run for your pipeline.  Download and take a look inside to get a better understanding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported pipeline definition to compute-pi.zip\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "### DON'T EDIT:                             ###\n",
    "### Create the pipeline description for kfp ###\n",
    "###############################################\n",
    "from kfp import compiler\n",
    "experiment_yaml_zip = EXPERIMENT_NAME + '.zip'\n",
    "compiler.Compiler().compile(\n",
    "    compute_pi,\n",
    "    experiment_yaml_zip\n",
    ")\n",
    "print(f\"Exported pipeline definition to {experiment_yaml_zip}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE on reusability and flexibility:** We defined our samples by setting a global variable (`SAMPLES`) and using it in `compute_pi` (the function that defines our pipeline) to make our `seeds`.  This makes our example simple, but has a strong downside: the number (and value of) our `seeds` is fixed.  If we rerun the pipeline twice it will give the exact same answer, and if we want to say compute Pi with `SAMPLES * 2` seeds, we need to rerun our notebook and create a new YAML.  A more flexible way to do this would be to create the seeds at runtime in the pipeline (have a simple component that takes `SAMPLES` and returns the seeds), but that is beyond the scope of this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ready to roll! Let's run this pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create Kubeflow Pipelines experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/9b336825-fd42-4535-a5a4-1d11a171c833\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###################################\n",
    "### DON'T EDIT:                 ###\n",
    "### Create the Experiment       ###\n",
    "###################################\n",
    "import kfp\n",
    "client = kfp.Client()\n",
    "exp = client.create_experiment(name=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And submit our pipeline to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/2d3caa99-e54b-4115-8b00-02cd49a73e82\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###############################################\n",
    "### DON'T EDIT:                             ###\n",
    "### Run the pipeline                        ###\n",
    "###############################################\n",
    "import time\n",
    "run = client.run_pipeline(\n",
    "    exp.id,\n",
    "    EXPERIMENT_NAME + '-' + time.strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    EXPERIMENT_NAME + '.zip',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the pipeline running, click the link above.  To access the returned data from the Average step, click on that step in the pipeline and look in the output artifact as shown below.\n",
    "\n",
    "![pipeline with results](images/kf-pipeline_with_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this method of returning a result (accessible in our browser) is likely not that useful for most problems.  See other other demos for saving results to minio or other locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why use a Map-Reduce Pattern?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow here splits all the samping up into many small pieces, does them all in isolation, then collects the results back up.  Why did we structure it that way, instead of say running a single component `sample-n` that loops to generate n samples in series?  We do that to take advantage of our flexible compute resources.\n",
    "\n",
    "Consider a case where we have a very expensive *sample* step and each sample takes 10 minutes to compute (instead of <1s here).  If we make a `sample-n` operation, that process looping through just 12 random points would take 2 hours to run!  But because of the nature of our horizontal map process, we can split those N *sample* steps up and run them in parallel.  This way we split N *sample* operations across N compute nodes in parallel, and can generate all N samples in the same time it takes to generate one.  \n",
    "\n",
    "This sort of pattern lets us horizontally scale our resources and is perfectly suited if you are doing the same sort of thing repetitively on isolated data.  You take advantage of the flexible compute environment to burst up to high usage for a short amount of time, then scale back down automatically.  You could apply the same strategy in other settings too, such as for computing new features from your data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
