{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and deploy Kubeflow and Pachyderm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the `mnist.npz` file to a blank directory on your local machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Pachyderm repo called `input-repo`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pachctl create repo input-repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check-in our `mnist.npz` to `input-repo`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pachctl put file input-repo@master:/mnist.npz -f mnist.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command copies the minst dataset from your local machine to the Pachyderm repo `input-repo` and Pachyderm will assign it a commit ID. Congratulations! Your data now has a HEAD commit, and Pachyderm has begun version-controlling the data!\n",
    "\n",
    "Confirm that the data is checked-in by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pachctl list file input-repo@master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Pachyderm  pipeline to trigger the new 'mnist_pipeline':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pachctl create pipeline -f pipeline_steps/pachyderm/pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pachyderm pipeline starts, creates a Kubeflow run, and passes data to the newly-created Kubeflow workers. Also, you will see an output commit, containing the model parameters, with the input commit and trigger pipeline in its provenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pachctl list pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pachctl list job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pachctl list repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the lowest level, the `s3` and `s3_out` fields of Pachyderm's pipeline specs support this integration. If a Kubeflow job can read and write data to and from an S3 bucket, then its data lineage can be tracked with Pachyderm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the Kubeflow UI, including the new `mnist_pipeline` created for this codelab, proceed to https://kubeflow.example.ca"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bit77f460b6b17a47f2a7dd300b92b9c9e5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}